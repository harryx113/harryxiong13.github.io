<!doctype html>
<html>
    <head>
        <title>Harry Xiong</title>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
        <link rel="stylesheet" href="https://use.typekit.net/fwo1rup.css">
        <link rel="stylesheet" type="text/css" href="style2.css">
    </head>

    <body>

        <nav class="navbar navbar-expand-lg navbar-light bg-light">
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a class = "nav" href = "https://harry-xiong.com">home</a>
                    </li>
                    <li class="nav-item">
                        <a class = "nav" href = "https://harry-xiong.com/#about-me">about</a>
                    </li>
                    <li class="nav-item">
                        <a class = "nav" href="https://harry-xiong.com/#intro-page">resume</a>
                    </li>
                    <li class="nav-item">
                        <a class = "nav" href = "https://harry-xiong.com/#ds-projects">projects</a>
                    </li>
                </ul>
            </div>
        </nav>

        <div class = "content">

            <h1>Speech Classification</h1>
            <h2 class = "project-date">May, 2020</h2>
            <h2 class = "project-technology-section">
                <span class = project-technology>Deep Learning</span>
                <span class = project-technology>Speech Recognition</span>
                <span class = project-technology>PyTorch</span>
                <span class = project-technology>Google Colab</span>
            </h2>
            <a href="../assets/Speech Classification.pdf">written report</a>
            <img src="../assets/speech-overall.jpeg" class = "project-pic">
            <span class = "project-description">
              Gender classification tasks in speech have been extensively studied. We propose, on the other hand, proposes to
              juxtapose gender classification results of African and Asian speakers and investigate whether any difference exists.
              The current study is also inspired by a growing body of literature on racial and gender stereotypes in social science fields.
            </span>
            <img src="../assets/mfcc.jpeg" class = "project-pic">
            <span class = "project-description">
              In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on
              a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC.
              In this project, 2140 same English speech samples of speakers from 177 countries with 214 different native languages are used as the data set.
              MP3 files are transformed to Mel-Frequency Cepstral Coefficients (MFCCs) using the librosa library with a sampling rate of 16000 fs.
              40 MFCCs are extracted for each sample and data is padded for the CNN model.
            </span>
            <img src="../assets/speech-models.png" class = "project-pic">
            <span class = "project-description">
              We trained 7 models in order to ensure any difference in error rate, if any, would be indeed persistent and not coincidental.
              The models that we selected were k-NN, Naive Bayes, SVM, linear perceptron, logistic regression, artificial
              neural networks (ANN), and convolutional neural networks (CNN). For each model, we tuned the hyperparameters, if
              any, until a validation set (consisting of White speakers)accuracy of near 90% is reached.
            </span>
            <img src="../assets/speech-cnn.png" class = "project-pic">
            <span class = "project-description">
              We were able to find the optimal number of channels to include
              in each convolutional layer to be 16 and 32 respectively. The
              convolutional layers are followed by a fully connect layer
              and finally a softmax layer. The model achieved over 97%
              accuracy on the validation set and as we will see, around
              93% accuracy on the test set.
            </span>
            <img src="../assets/speech-accuracies.png" class = "project-pic">
            <span class = "project-description">
              What we observed is that in general, there is no observable difference in the error rate on the Asian test set compared to the
              baseline White test set. For some models, the performance on the Asian test set is even better. On the other hand, all models
              predict less accurately on the African test set, compared to on the Asian counterpart. Further, except for the SVM model, the test set accuracy is
              lower for African speakers compared to the baseline White speakers. The results suggest that there is significant potential bias when classifying
              genders with African accents.
            </span>
            <img src="../assets/speech-precision&recall.png" class = "project-pic">
            <span class = "project-description">
              Precision and recall results can be found here.
            </span>

        </div>

    </body>
</html>
